import os, re, json, torch, whisper, dateparser
from datetime import datetime, timedelta
from typing import List, Optional
from pydantic import BaseModel, Field, ValidationError
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from pprint import pprint

# ===================== 1. ëª¨ë¸ ë° í™˜ê²½ ì¤€ë¹„ =====================

# âš ï¸ GPT ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ API í‚¤ëŠ” í•„ìš”í•©ë‹ˆë‹¤.
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")


AUDIO_FILE = "wav.file/10ì›” 26ì¼ íšŒì˜ë¡.wav"

torch.cuda.empty_cache()
print("âœ… í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ")

# ===================== 2. ë°ì´í„° êµ¬ì¡° ì •ì˜ (ì•¡ì…˜ ì•„ì´í…œë§Œ) =====================

class ActionItem(BaseModel):
    name: str = Field(..., description="ë‹´ë‹¹ì ì´ë¦„")
    task: str = Field(..., description="í•  ì¼")
    due: Optional[str] = Field(None, description="YYYY-MM-DD ë˜ëŠ” null")

# ===================== 3. Whisper STT (ìŒì„± ì¸ì‹) =====================

# ğŸ™ï¸ Whisper ëª¨ë¸ì€ ì—¬ì „íˆ í•„ìš”í•©ë‹ˆë‹¤.
model = whisper.load_model("large-v3")
print("ğŸ™ï¸ Whisper ë³€í™˜ ì¤‘...")

if not os.path.exists(AUDIO_FILE):
    raise FileNotFoundError(f"âŒ íŒŒì¼ ì—†ìŒ: {AUDIO_FILE}")

result = model.transcribe(AUDIO_FILE, language="ko")
full_text = result["text"].strip()
print("âœ… ë³€í™˜ ì™„ë£Œ. í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n", full_text[:300])


# ===================== 4. ê¸°ë³¸ í•¨ìˆ˜ ì •ì˜ (Ollama ì§€ì› ë° JSON ì²˜ë¦¬) =====================

def safe_llm_json(llm, prompt_text, retries=2):
    """LLMì´ JSONì„ ê¹¨ëœ¨ë¦¬ë©´ ìë™ ì¬ì‹œë„"""
    for i in range(retries + 1):
        resp = llm.invoke(prompt_text)
        text = resp.content.strip()
        # JSON ë³¸ë¬¸ë§Œ ì¶”ì¶œ
        json_part = re.search(r'\{[\s\S]*\}', text)
        if not json_part:
            print(f"âš ï¸ JSON ë³¸ë¬¸ ë¯¸ê²€ì¶œ ({i+1}/{retries+1}) â†’ ì¬ì‹œë„")
            continue
        try:
            return json.loads(json_part.group(0))
        except json.JSONDecodeError:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ ({i+1}/{retries+1}) â†’ ì¬ì‹œë„")
            # í”„ë¡¬í”„íŠ¸ì— JSONë§Œ ì¶œë ¥í•˜ë„ë¡ ì¬ìš”ì²­
            prompt_text += "\n\nJSON ì™¸ ë¬¸ì¥ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ê³  ìœ íš¨í•œ JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”."
    raise ValueError("âŒ LLMì´ ìœ íš¨í•œ JSONì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


def normalize_due(due_text: Optional[str], base_dt: datetime) -> Optional[str]:
    """ìƒëŒ€ ë‚ ì§œë¥¼ YYYY-MM-DDë¡œ ì •ê·œí™”."""
    if not due_text: return None

    s = str(due_text).strip()
    current_year = base_dt.year

    # YYYY-MM-DD í˜•ì‹ ì´ë¯¸ ì¶©ì¡± ì‹œ
    if re.match(r"\d{4}-\d{2}-\d{2}", s):
        try: return datetime.strptime(s, "%Y-%m-%d").strftime("%Y-%m-%d")
        except ValueError: return None

    if s in {"ë¯¸ì •", "ë¶ˆëª…", "null", "ì—†ìŒ", ""}: return None
    if s in {"ì˜¤ëŠ˜", "ì˜¤ëŠ˜ ì¤‘"}: return base_dt.strftime("%Y-%m-%d")
    if s == "ë‚´ì¼": return (base_dt + timedelta(days=1)).strftime("%Y-%m-%d")

    # dateparserë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ íŒŒì‹±
    parsed = dateparser.parse(
        s,
        languages=["ko"],
        settings={"RELATIVE_BASE": base_dt, "PREFER_DATES_FROM": "past"},
    )
    
    if any(word in s for word in ["ë‚´ì¼", "ë‹¤ìŒ", "ì´ë²ˆ ì£¼", "ì´ë²ˆì£¼", "ê¹Œì§€"]):
        parsed_future = dateparser.parse(
            s,
            languages=["ko"],
            settings={"RELATIVE_BASE": base_dt, "PREFER_DATES_FROM": "future"},
        )
        if parsed_future: parsed = parsed_future

    if parsed:
        if parsed.year < current_year and parsed.date() < base_dt.date():
            parsed = parsed.replace(year=current_year + 1)
        elif parsed.year < current_year:
            parsed = parsed.replace(year=current_year)
            
        return parsed.strftime("%Y-%MM-%d")

    return None


def extract_actions_and_normalize(llm_model_name: str, action_candidates: List[str], base_dt: datetime):
    """íŠ¹ì • LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì•¡ì…˜ ì•„ì´í…œì„ ì¶”ì¶œí•˜ê³  ë‚ ì§œë¥¼ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\n--- ğŸ§  {llm_model_name} ëª¨ë¸ë¡œ ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ ì‹œì‘ ---")
    
    # ğŸŒŸ ëª¨ë¸ ê³„ì—´ì— ë”°ë¼ Chat í´ë˜ìŠ¤ ì„ íƒ
    if "gpt" in llm_model_name:
        llm = ChatOpenAI(model_name=llm_model_name, temperature=0)
    elif "ollama" in llm_model_name:
        # 'ollama-' ì ‘ë‘ì‚¬ë¥¼ ì œê±°í•˜ê³  Ollama ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ChatOllama ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        ollama_model = llm_model_name.replace("ollama-", "")
        llm = ChatOllama(model=ollama_model, temperature=0)
    else:
        print(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ê³„ì—´: {llm_model_name}. ChatOpenAIë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    # Ollama ëª¨ë¸ì— ì í•©í•œ JSON ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (JSONë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì¡°)
    fallback_prompt = f"""
    ë„ˆëŠ” íšŒì˜ ì¤‘ ì•¡ì…˜ì•„ì´í…œì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    **ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ ì—†ì´, ì˜¤ì§ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.**
    ì•„ë˜ ë¬¸ì¥ë“¤ì—ì„œ ë‹´ë‹¹ì(name), í•  ì¼(task), ê¸°í•œ(due)ì„ ì¶”ì¶œí•˜ì—¬ JSON ë°°ì—´ë¡œ ë§Œë“œì„¸ìš”.
    ê¸°í•œì´ ì—†ìœ¼ë©´ 'null'ë¡œ ë‘¡ë‹ˆë‹¤. 'due'ëŠ” ìƒëŒ€ì ì¸ ë‚ ì§œ í‘œí˜„ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
    
    ì¶œë ¥ í˜•ì‹:
    {{
      "action_items": [
        {{"name": "ë‹´ë‹¹ì", "task": "í•  ì¼", "due": "ê¸°í•œ í…ìŠ¤íŠ¸ or null"}}
      ]
    }}
    
    ë¬¸ì¥:
    {json.dumps(action_candidates, ensure_ascii=False, indent=2)}
    """
    
    try:
        # ì—¬ê¸°ì„œ LLM í˜¸ì¶œ
        fallback_json = safe_llm_json(llm, fallback_prompt)
    except ValueError as e:
        print(f"âŒ {llm_model_name} ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

    action_items = fallback_json.get("action_items", [])
    
    # ë‚ ì§œ ì •ê·œí™”
    for item in action_items:
        due_raw = item.get("due")
        if due_raw:
            clean_due = re.sub(r'(ê¹Œì§€|ì¤‘|ë¶€í„°)', '', str(due_raw))
            clean_due = re.sub(r'(ë‹¤ìŒì£¼)([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼])', r'\1 \2', clean_due)
            item["due"] = normalize_due(clean_due, base_dt)
            
    print(f"âœ… {llm_model_name} ì•¡ì…˜ ì•„ì´í…œ {len(action_items)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    return action_items


# ===================== 5. ì•¡ì…˜ì•„ì´í…œ í›„ë³´ íƒì§€ =====================

action_candidates = []
for line in full_text.split("\n"):
    # ì•¡ì…˜ ì•„ì´í…œì´ ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë¬¸ì¥ë§Œ í•„í„°ë§
    if any(k in line for k in ["ê¹Œì§€", "í•´ì•¼", "ê²°ì •", "ì™„ë£Œ", "ì§„í–‰", "ì‘ì„±", "ê²€í† "]):
        action_candidates.append(line.strip())

print(f"\nğŸ“‹ ì•¡ì…˜ ë¬¸ì¥ í›„ë³´ {len(action_candidates)}ê°œ íƒì§€ë¨")

# ===================== 6. ëª¨ë¸ë³„ ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ ë° ë¹„êµ =====================

base_dt = datetime.now()

# ğŸ§ª ëª¨ë¸ A: gpt-4o-mini (í´ë¼ìš°ë“œ/ìœ ë£Œ ë¹„êµêµ°)
MODEL_A = "gpt-4o-mini"
action_items_A = extract_actions_and_normalize(MODEL_A, action_candidates, base_dt)

# ğŸ§ª ëª¨ë¸ B: Ollama (ë¡œì»¬/ë¬´ë£Œ ë¹„êµêµ°)
# ğŸŒŸ ì‚¬ìš©ìë‹˜ì˜ Ollama list ê²°ê³¼ì— ë§ì¶° íƒœê·¸ ë³€ê²½: exaone3.5:7.8b
MODEL_B = "ollama-exaone3.5:7.8b" 
action_items_B = extract_actions_and_normalize(MODEL_B, action_candidates, base_dt)


# ===================== 7. ìµœì¢… ê²°ê³¼ ë¹„êµ ë° ë°¸ë¦¬ë°ì´ì…˜ =====================

# gpt-4o-mini ê²°ê³¼ë¥¼ ìµœì¢…ìœ¼ë¡œ ì„ íƒí•˜ê³ , Ollama ê²°ê³¼ì™€ ë¹„êµ ì¶œë ¥
final_action_items = action_items_A

print("\n" + "="*50)
print("ğŸ¯ ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ ê²°ê³¼ ë¹„êµ")
print("="*50)

print(f"\n[ëª¨ë¸ A: {MODEL_A} - {len(action_items_A)}ê°œ]")
pprint(action_items_A, indent=4)

print(f"\n[ëª¨ë¸ B: {MODEL_B} - {len(action_items_B)}ê°œ]")
pprint(action_items_B, indent=4)
print("\n" + "="*50)

try:
    # ActionItem í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° êµ¬ì¡° ë°¸ë¦¬ë°ì´ì…˜ (ëª¨ë¸ A ê¸°ì¤€)
    validated_items = [ActionItem(**item) for item in final_action_items]
    print(f"\nâœ… ë°¸ë¦¬ë°ì´ì…˜ í†µê³¼ (ìµœì¢… ì‚¬ìš© ëª¨ë¸: {MODEL_A}) - ì¶”ì¶œëœ ì•¡ì…˜ ì•„ì´í…œ {len(validated_items)}ê°œ")
except ValidationError as e:
    print("âŒ ë°¸ë¦¬ë°ì´ì…˜ ì‹¤íŒ¨:", e)
    raise

print("\n--- ìµœì¢… ê²°ê³¼ JSON (gpt-4o-mini ê¸°ì¤€) ---")
print(json.dumps([item.model_dump() for item in validated_items], indent=4, ensure_ascii=False))
